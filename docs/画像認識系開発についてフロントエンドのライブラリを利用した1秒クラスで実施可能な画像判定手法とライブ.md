<img src="https://r2cdn.perplexity.ai/pplx-full-logo-primary-dark%402x.png" style="height:64px;margin-right:32px"/>

# 画像認識系開発についてフロントエンドのライブラリを利用した1秒クラスで実施可能な画像判定手法とライブラリ

フロントエンドだけで「1秒以内クラス」で動く画像判定なら、軽量モデル＋WebGL/WebGPU/Wasｍ対応ライブラリを使ったクライアント推論が現実的です。[^1_1][^1_2]

## 想定ユースケースと前提

- 1枚画像の「クラス分類」（例: 犬/猫判定、ラベル付け）を 1 秒以内で返す。
- ブラウザ上（React/Vue/生JSなど）で完結し、サーバー推論は使わない。
- モバイル端末（中堅スマホ）でも実用速度を狙う。

この前提なら、下記のどれかを選ぶ形になります。[^1_3][^1_1]

***

## TensorFlow.js 系

### 特徴

- JS界隈で情報量が多く、学習コストが低い。
- MobileNet など軽量モデルをブラウザにロードして、画像分類を即実行可能。ImageNet 1000クラス分類などのデモも多数。[^1_4][^1_1]


### 主な選択肢

- **TensorFlow.js** 本体
    - 利用例: `@tensorflow-models/mobilenet` で `model.classify(image)` を呼ぶだけで 1枚画像のクラス予測が可能。[^1_5][^1_1][^1_4]
    - WebGL バックエンド利用で、そこそこのスマホでも 100〜200ms 程度の推論例が報告されており、1秒以内は現実的。[^1_5][^1_3]


### 向いている場面

- React / Vue / 素の JS で簡単に組み込みたい。
- まずは完成度の高いサンプルから始めたい。

***

## MediaPipe / Google AI Edge (Image Classifier)

### 特徴

- Google の Edge / MediaPipe ソリューションとして提供される「Image Classifier」タスクをそのまま Web で利用可能。[^1_6]
- 画像の前処理（リサイズ、正規化など）や後処理を内部で処理してくれるため、コード量を少なくできる。[^1_6]


### パフォーマンス

- WebAssembly や WebGL/WebGPU を利用した高速化に対応しており、「インタラクティブに扱える速度」を目標に設計されたライブラリ。[^1_6]
- 1枚画像の分類なら 1秒以内に十分収まる設計。[^1_6]


### 向いている場面

- Google 提供の安定したソリューションを使いたい。
- 前処理やラベル管理などをあまり自分で書きたくない。

***

## ONNX Runtime Web 系（WebGPU / WASM）

### 特徴

- 任意の ONNX モデル（PyTorch / TensorFlow 等から変換）をブラウザで動かせる。[^1_7][^1_8][^1_9]
- WebGPU / WebAssembly バックエンドで大きく高速化でき、CPUのみより数十倍高速になる報告あり。[^1_2][^1_8][^1_7]


### 代表的な構成

- **ONNX Runtime Web + WebGPU**
    - WebGPU 対応ブラウザなら GPU を使って推論を高速化。[^1_8][^1_7]
- **Next.js + ONNX Runtime Web** テンプレート
    - 公式チュートリアルで画像分類 Web アプリのテンプレートが提供されている。[^1_9]


### パフォーマンス

- WebGPU + 量子化モデルで、従来の CPU 実装と比べて 20〜550倍の高速化事例（背景除去タスク）もある。[^1_2]
- 軽量な分類モデルなら、ブラウザ側での単発推論は 1秒以内どころか数十〜数百 ms 程度も狙える。[^1_7][^1_2]


### 向いている場面

- 既に自前で学習したモデル（ONNX形式）を持っている。
- 速度と柔軟性を両立したい（ブラウザでも本格運用したい）。

***

## ライブラリ比較（ブラウザ画像判定）

| 観点 | TensorFlow.js | MediaPipe Image Classifier | ONNX Runtime Web |
| :-- | :-- | :-- | :-- |
| 実装難易度 | 比較的簡単。[^1_1][^1_4] | 非常に簡単（前処理込み）。[^1_6] | 中〜上級（ONNXモデル準備が必要）。[^1_7][^1_9] |
| 対応モデル | 公式プリセット多数（MobileNet など）。[^1_1][^1_4] | 提供モデル＋カスタムモデル（タスク指向）。[^1_6] | ONNX なら何でも（分類・検出・セグメント）。[^1_8][^1_9] |
| 高速化バックエンド | WebGL / WASM。[^1_1][^1_4] | WASM＋GPU 対応。[^1_6] | WebGPU / WASM / WebGL。[^1_7][^1_8] |
| 1秒以内の実現性 | 軽量モデルなら十分可能。[^1_1][^1_5][^1_3] | 単発判定ならほぼ問題なし。[^1_6] | モデル次第だが最も高速。大きめモデルでも現実的。[^1_2][^1_7] |
| カスタムモデルのしやすさ | tf.js で再学習 or 変換。[^1_1][^1_3] | タスクに沿った形での利用が前提。[^1_6] | 学習→ONNX変換→ブラウザ配備の流れ。[^1_8][^1_9] |


***

## 1秒クラスで動かすための実装ポイント

- 軽量モデルを選ぶ
    - MobileNet / EfficientNet-lite など、パラメータが少ない分類モデルを採用。[^1_1][^1_4][^1_9]
- 入力サイズを絞る
    - 224×224 〜 320×320 程度に縮小して推論（それ以上に大きいと一気に重くなる）。[^1_4][^1_1]
- WebGL / WebGPU / WASM を有効化
    - TensorFlow.js なら WebGL バックエンド、ONNX Runtime Web なら WebGPU or WASM を有効にする。[^1_8][^1_2][^1_7]
- 単発推論に特化した UI
    - カメラの常時ストリームではなく「1枚キャプチャして判定」方式にすると、処理負荷を抑えつつ体感も速くなる。[^1_5]

***

もし、

- 対象クラス数（例: 2クラスか100クラスか）、
- 使用予定フレームワーク（React / Vue / Next.js など）、
- 対象デバイス（PC中心かスマホ中心か）

を教えてもらえれば、それに合わせて「このライブラリ＋このモデル＋だいたいこれくらいのコード」というレベルまで具体的に提案できます。
<span style="display:none">[^1_10]</span>

<div align="center">⁂</div>

[^1_1]: https://www.freecodecamp.org/news/image-classification-in-the-browser-with-javascript-bec7b5a7a8c3/

[^1_2]: https://img.ly/blog/browser-background-removal-using-onnx-runtime-webgpu/

[^1_3]: https://ceur-ws.org/Vol-4044/paper01.pdf

[^1_4]: https://www.tensorflow.org/js/models?hl=ja

[^1_5]: https://www.linkedin.com/pulse/real-time-image-classification-tensorflow-daniel-wind

[^1_6]: https://ai.google.dev/edge/mediapipe/solutions/vision/image_classifier/web_js

[^1_7]: https://onnxruntime.ai/docs/tutorials/web/ep-webgpu.html

[^1_8]: https://techcommunity.microsoft.com/blog/educatordeveloperblog/use-webgpu--onnx-runtime-web--transformer-js-to-build-rag-applications-by-phi-3-/4190968

[^1_9]: https://onnxruntime.ai/docs/tutorials/web/classify-images-nextjs-github-template.html

[^1_10]: https://github.com/zanllp/sd-webui-infinite-image-browsing


---

# PC中心のブラウザ動作でVueを使用。求めているのはこれらの画像をもとに特定の部位の判定です

Vue＋PCブラウザ前提で、添付のような画面から「特定の部位（例: 先攻/後攻表示、勝敗表示など）」だけを 1 秒以内で判定するなら、次の構成が現実的です。[^2_1][^2_2][^2_3]

## 方針イメージ

- 対象:
    - コイン画面から「自分/相手どちらが先攻になったか」。
    - リザルト画面から「WIN/LOSE」やプレイヤー名など。
- 手法:

1. 画像の中から“判定したい部位”だけを切り出す（座標でクロップ）。
2. その小さい画像を **画像分類モデル or OCR** で判定。
- フロントのみで完結し、1 枚あたり数百 ms 程度で処理する構成を取る。

***

## Vue で扱いやすいライブラリ候補

### 1. TensorFlow.js（画像分類系）

- Vue からそのまま npm で導入しやすく、サンプルも多い。
- 手順イメージ
    - 先攻表示部分だけを 224×224 くらいで `<canvas>` に描画して `tf.browser.fromPixels` で張り付け。
    - 事前に学習した軽量モデル（自作 or MobileNet の転移学習）で `model.predict()`。
- 先攻/後攻、WIN/LOSE のような 2〜数クラス分類なら、軽量モデル＋WebGL で 100〜200ms レベルが十分期待できる。


### 2. ONNX Runtime Web（独自モデルを本格運用したい場合）

- Python 側で学習 → ONNX に変換 → ブラウザで ONNX Runtime Web＋WebGPU/WASM で推論。
- 画面から切り出した小さなパッチを入力すれば、やはり数十〜数百 ms 程度で判定可能。
- クラス数が増えたり将来他の部位判定を追加したい場合にも拡張しやすい。

***

## 部位判定の具体的な設計ポイント

- 画像座標を固定する
    - ゲーム UI が固定レイアウトなので、「コイン中央の 〇〇×〇〇 付近」「WIN/LOSE テキスト付近」などあらかじめ座標を決めてクロップする。
- 入力サイズを小さく保つ
    - 224×224 or 128×128 程度に縮小して推論すると高速・安定。
- 判定タスクごとにモデルを分けるか、1モデル多クラスにするか
    - 例:
        - モデルA: 先攻/後攻/未決定
        - モデルB: WIN/LOSE/DRAW

***

## 追加で確認したいこと

- 判定したい「部位」の種類（先攻/後攻・勝敗・プレイヤー名・ランクバッジなど、どこまで自動化したいか）。
- 学習用画像をどれくらい用意できそうか（スクショ枚数、ラベル付け可能か）。

これが分かれば、「TensorFlow.js＋Vue でこのくらいのコード構成」「ONNX Runtime Web を使うならこの設計」という形で、もう少し踏み込んだ実装案を出せます。

<div align="center">⁂</div>

[^2_1]: raw-result-lose-1800p.jpg

[^2_2]: raw-coin-win-1800p.jpg

[^2_3]: raw-coin-lose-1800p.jpg


---

# VICTORY/LOSEの文字、対戦相手が選択しています／先行・後攻を選択してください

の判定です

VICTORY/LOSE や「対戦相手が選択しています」「先行・後攻を選択してください」のような固定テキストなら、Vue＋ブラウザのみでも画像分類 or OCR で 1 秒以内判定が十分可能です。[^3_1][^3_2][^3_3][^3_4][^3_5][^3_6]

***

## どの手法を使うか

- **画像分類モデル方式（推奨）**
    - 各テキスト部分をそのまま「画像として」判定する。
    - クラス例: `["VICTORY","LOSE","NONE"]`, `["WAITING","CHOOSE_FIRST_SECOND","NONE"]` など。
    - 文字デザインやエフェクトが固定で、フォントもゲーム側で一定なので、少ない学習データでも高精度になりやすい。
- OCR（Tesseract.js など）
    - 実際の文字列を読み取る方法。
    - 日本語と英語が混ざる UI でも汎用的だが、ゲーム画面のような装飾文字は誤読しやすく、速度も画像分類よりやや遅くなりがち。
    - 「VICTORY/LOSE」程度なら分類、長文や可変テキストを読みたい場合に OCR を併用するイメージ。[^3_7][^3_5][^3_8]

***

## Vue での実装イメージ（画像分類）

1. **座標の決定とクロップ**
    - 1800p スクショを基準に、
        - VICTORY/LOSE 部分
        - 「対戦相手が選択しています」テキスト部分
        - 「先行・後攻を選択してください」テキスト部分
の矩形座標を一度だけ決めておく（ゲーム UI は固定レイアウトなので）。[^3_2][^3_3][^3_1]
    - `<canvas>` に全体画像を描画してから、`drawImage` でその矩形だけ別キャンバスに切り出す。
2. **TensorFlow.js＋Vue**
    - 例: `@tensorflow/tfjs` を導入し、Vue コンポーネント内でモデルをロード。
    - 切り出したキャンバスを `tf.browser.fromPixels` → `resizeNearestNeighbor([128,128])` → 正規化 → `model.predict`。[^3_4][^3_9][^3_10]
    - 出力の最大スコアのクラスに応じて、
        - `VICTORY` or `LOSE`
        - `WAITING_FOR_OPPONENT` or `CHOOSE_FIRST_SECOND` or `NONE`
を状態として扱う。
3. **学習方法（オフラインで実施）**
    - 手元で Python＋TensorFlow / PyTorch などを使い、
        - VICTORY 画像パッチ数百枚
        - LOSE 画像パッチ数百枚
        - それ以外（背景）数百枚
を集めて転移学習（MobileNetV2 など）。[^3_11][^3_12]
    - 学習済みモデルを tfjs or ONNX 形式に変換してビルド成果物に同梱する。

***

## ONNX Runtime Web＋Vue を使う場合

- より高速＆柔軟にしたければ、学習済みモデルを ONNX にして **ONNX Runtime Web** で推論する。
- Vue との組み合わせ例やコードは、日本語記事やデモリポジトリが既にあり、WebGL/WebGPU バックエンドでの画像分類が紹介されている。[^3_12][^3_6]
- 使い方自体は TensorFlow.js と似ており、切り出したキャンバス画像をテンソル化して `session.run` するだけ。

***

## 実装時に気を付ける点

- 入力サイズは 128〜224 辺りに固定し、解像度を上げすぎない。
- 判定を連打しないよう、
    - 画面変化後 200〜300ms だけ待ってから 1 回だけ推論する、
    - 同じフレームで複数判定が走らないようロックを入れる。
- ゲーム側の UI アップデートで座標がずれた時のため、座標を設定ファイル化しておくとメンテしやすい。

***

もし可能なら、

- 1800p 以外の解像度でも動かしたいか
- 学習環境（Python 可否）
を教えてもらえれば、「どのライブラリで、どの手順で学習〜Vue 組み込みまでやるか」をもう少し具体的なステップに落とし込めます。
<span style="display:none">[^3_13]</span>

<div align="center">⁂</div>

[^3_1]: raw-result-lose-1800p.jpg

[^3_2]: raw-coin-win-1800p.jpg

[^3_3]: raw-coin-lose-1800p.jpg

[^3_4]: https://eralpozcan.dev/blog/how-to-use-object-recognition-with-tensorflowjs-and-vue-3

[^3_5]: https://qiita.com/tinymouse/items/ec3a6dfae0b334c95894

[^3_6]: https://rest-term.com/archives/3655/

[^3_7]: https://zenn.dev/qwegat/articles/5b81b54e36fd11

[^3_8]: https://stackoverflow.com/questions/71765904/ocr-scanner-from-image-to-text-using-vue-js

[^3_9]: https://dev.to/mohsenkamrani/tensorflowjs-for-beginners-image-classification-4aaj

[^3_10]: https://github.com/GabrielAlfs/img-classifier

[^3_11]: https://www.codementor.io/@chidoziemanagwu880043/how-to-build-a-simple-image-classification-api-using-tensorflow-and-flask-2n84de1nt0

[^3_12]: https://github.com/microsoft/onnxruntime-web-demo

[^3_13]: https://onnxruntime.ai/docs/tutorials/web/classify-images-nextjs-github-template.html


---

# 複数解像度で実施したいです。具体的には1280x720～3840x2160の可変でウィンドウサイズに合わせて判定します

解像度が 1280×720～3840×2160 で可変でも、**座標を比率で管理してからクロップする**構成にすれば 1 秒以内判定は維持できます。[^4_4]

## 解像度可変時の基本方針

- 判定したい領域（VICTORY/LOSE、待機テキスト、先攻/後攻テキスト）を
    - 「基準解像度上の比率（0～1）」で持つ
    - 例: 画面幅の 40～60%、高さの 20～30% など
- 実際のウィンドウサイズ取得時に
    - `x = ratioX * currentWidth`
    - `y = ratioY * currentHeight`
    - `w = ratioW * currentWidth`
    - `h = ratioH * currentHeight`
でクロップ矩形を計算して `<canvas>` に切り出す。[^4_4]

***

## Canvas とスケーリングの扱い

- `<canvas>` の `width` / `height` は **論理サイズ**、CSS の幅高さは表示サイズとして分ける。
- `window.devicePixelRatio` を考慮して、
    - `canvas.width  = viewWidth  * devicePixelRatio`
    - `canvas.height = viewHeight * devicePixelRatio`
    - `ctx.scale(devicePixelRatio, devicePixelRatio)`
のようにしておくと、高 DPI でも文字がつぶれにくいクロップができる。[^4_4]
- 切り出し後は、モデル入力用に 128×128 や 224×224 など固定サイズへリサイズしてから推論する。

***

## 複数解像度対応でのモデル設計

- すべての解像度で撮った画面からサンプルを作り、**同じ入力サイズにリサイズして学習**すれば 1 つのモデルで対応可能。
- 特に文字位置や比率がほぼ同じなら、
    - 1280×720 ～ 3840×2160 を混ぜた学習データでも問題なく汎化することが多い。
- 判定ラベルは前回と同様、
    - `VICTORY / LOSE / NONE`
    - `WAITING_FOR_OPPONENT / CHOOSE_FIRST_SECOND / NONE`
など少数クラスにしておくと学習も軽く、推論も高速。

***

## Vue 実装の流れ（解像度対応込み）

1. ウィンドウサイズ取得（`window.innerWidth`, `innerHeight` またはゲーム Canvas サイズ）
2. 比率から判定領域の `x, y, w, h` を算出
3. 高 DPI 用に `devicePixelRatio` でスケーリングした Canvas に画面を描画し、その領域をサブ Canvas に `drawImage` で切り出し。[^4_4]
4. サブ Canvas をモデル入力サイズにリサイズし、TensorFlow.js or ONNX Runtime Web で推論。
5. 結果クラスを状態に反映（VICTORY/LOSE など）。

この方式なら、フル HD でも 4K でも「比率＋リサイズ」で同じ判定ロジックを流用できるので、ウィンドウサイズ連動の可変判定がやりやすくなります。
<span style="display:none">[^4_1][^4_10][^4_11][^4_12][^4_13][^4_2][^4_3][^4_5][^4_6][^4_7][^4_8][^4_9]</span>

<div align="center">⁂</div>

[^4_1]: raw-result-lose-1800p.jpg

[^4_2]: raw-coin-win-1800p.jpg

[^4_3]: raw-coin-lose-1800p.jpg

[^4_4]: https://www.cyokodog.net/blog/pixijs-retina-display/

[^4_5]: https://blog.capilano-fw.com/?p=3578

[^4_6]: https://zenn.dev/mmomm/articles/8dddc19f7ca786025417

[^4_7]: https://qiita.com/olt/items/843934572a1208458758

[^4_8]: https://www.issoh.co.jp/tech/details/3608/

[^4_9]: https://qiita.com/alt_yamamoto/items/2fbd63ee77d87dd727f7

[^4_10]: https://tech-blog.optim.co.jp/?page=1619141400

[^4_11]: https://future-architect.github.io/arch-guidelines/documents/forWebFrontend/web_frontend_guidelines.html

[^4_12]: https://notes.sharesl.net/articles/1601/

[^4_13]: https://note.com/yukikkoaimanabi/n/n48076458af06

